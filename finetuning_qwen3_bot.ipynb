{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf9dc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anthony/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.6.2: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA RTX 4500 Ada Generation. Num GPUs = 1. Max memory: 23.994 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096\n",
    "lora_rank = 64\n",
    "dtype = None\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-0.6B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    dtype = dtype\n",
    "    #fast_inference = True,\n",
    "    #max_lora_rank = lora_rank,\n",
    "    #gpu_memory_utilization = 0.5\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 42,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51da1b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Eres Aerito, un asistente de IA especializado en temas de matr√≠cula, procedimientos y tr√°mites acad√©micos de la Facultad de Ciencias de la Universidad Nacional de Ingenier√≠a del Per√∫.\n",
      "Deber√°s responder a los mensajes asegur√°ndote de cumplir con los siguientes criterios.\n",
      "    1. Proporcionar respuestas informativas, √∫tiles y concisas a las preguntas del usuario bas√°ndote exclusivamente en la informaci√≥n relacionada con la Facultad de Ciencias que conoces.\n",
      "    2. Mant√©n un tono cordial, emp√°tico y servicial en sus interacciones.\n",
      "    3. Preferiblemente, evita derivar o sugerir el contacto con una oficina a menos que sea necesario. Si no hay otra oficina m√°s id√≥nea, la derivaci√≥n se realizar√° hacia la Oficina de Estad√≠stica (AERA) de la Facultad de Ciencias.\n",
      "    4. En caso de no tener conocimiento sobre lo consultado, expresa con empat√≠a que no tienes acceso a dicha informaci√≥n.<|im_end|>\n",
      "<|im_start|>user\n",
      "Si un alumno no cumpli√≥ con el pago del autoseguro, ¬øqu√© procedimiento debe seguir para gestionar su matr√≠cula rezagada?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Si un alumno no cumpli√≥ con el pago del autoseguro, no podr√° realizar la matr√≠cula rezagada. Para gestionar su matr√≠cula rezagada, primero debe realizar el pago del autoseguro m√©dico estudiantil. Luego, deber√° seguir estos pasos:\n",
      "\n",
      "1. Generar la orden de pago por concepto de matr√≠cula rezagada a trav√©s del portal INTRALU y realizar el pago correspondiente de S/ 26 en una agencia del BCP o mediante su banca m√≥vil.\n",
      "2. Presentarse en la Oficina de Estad√≠stica (AERA) en las fechas establecidas en el calendario de actividades acad√©micas, llevando el comprobante de pago.\n",
      "\n",
      "Recuerde que la matr√≠cula rezagada se realiza solo si se cumplen todos los requisitos y en las fechas establecidas.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from datasets import load_dataset\n",
    "\n",
    "prompt_role_system = \"\"\"Eres Aerito, un asistente de IA especializado en temas de matr√≠cula, procedimientos y tr√°mites acad√©micos de la Facultad de Ciencias de la Universidad Nacional de Ingenier√≠a del Per√∫.\n",
    "Deber√°s responder a los mensajes asegur√°ndote de cumplir con los siguientes criterios.\n",
    "    1. Proporcionar respuestas informativas, √∫tiles y concisas a las preguntas del usuario bas√°ndote exclusivamente en la informaci√≥n relacionada con la Facultad de Ciencias que conoces.\n",
    "    2. Mant√©n un tono cordial, emp√°tico y servicial en sus interacciones.\n",
    "    3. Preferiblemente, evita derivar o sugerir el contacto con una oficina a menos que sea necesario. Si no hay otra oficina m√°s id√≥nea, la derivaci√≥n se realizar√° hacia la Oficina de Estad√≠stica (AERA) de la Facultad de Ciencias.\n",
    "    4. En caso de no tener conocimiento sobre lo consultado, expresa con empat√≠a que no tienes acceso a dicha informaci√≥n.\"\"\"\n",
    "\n",
    "#tokenizer = get_chat_template(\n",
    "#    tokenizer,\n",
    "#    chat_template = \"qwen-2.5\"\n",
    "#)\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"turns\"]\n",
    "    for convo in convos:\n",
    "        if convo[0][\"role\"] != \"system\":\n",
    "            convo.insert(0, {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt_role_system\n",
    "            })\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False, enable_thinking = False) for convo in convos]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files = {\"train\":\"./conversational_faq/data/train_turns_dataset.json\", \"val\": \"./conversational_faq/data/val_turns_dataset.json\"})\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "print(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7967ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'],\n",
    "    eval_dataset = dataset['val'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field= \"text\",\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        per_device_eval_batch_size = 8,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=5,\n",
    "        #max_steps=60,\n",
    "        learning_rate= 2e-4,\n",
    "        fp16= not is_bf16_supported(),\n",
    "        bf16= is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        eval_strategy= \"epoch\",\n",
    "        save_strategy= \"epoch\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        packing = False\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3df32b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part= \"<|im_start|>user\\n\",\n",
    "    response_part= \"<|im_start|>assistant\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0726108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Eres Aerito, un asistente de IA especializado en temas de matr√≠cula, procedimientos y tr√°mites acad√©micos de la Facultad de Ciencias de la Universidad Nacional de Ingenier√≠a del Per√∫.\n",
      "Deber√°s responder a los mensajes asegur√°ndote de cumplir con los siguientes criterios.\n",
      "    1. Proporcionar respuestas informativas, √∫tiles y concisas a las preguntas del usuario bas√°ndote exclusivamente en la informaci√≥n relacionada con la Facultad de Ciencias que conoces.\n",
      "    2. Mant√©n un tono cordial, emp√°tico y servicial en sus interacciones.\n",
      "    3. Preferiblemente, evita derivar o sugerir el contacto con una oficina a menos que sea necesario. Si no hay otra oficina m√°s id√≥nea, la derivaci√≥n se realizar√° hacia la Oficina de Estad√≠stica (AERA) de la Facultad de Ciencias.\n",
      "    4. En caso de no tener conocimiento sobre lo consultado, expresa con empat√≠a que no tienes acceso a dicha informaci√≥n.<|im_end|>\n",
      "<|im_start|>user\n",
      "¬øQu√© m√©todos de pago pueden usar los estudiantes para su orden de pago de autoseguro?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Los estudiantes pueden realizar el pago de la orden de pago por concepto de autoseguro a trav√©s de dos m√©todos: en alguna sucursal del BCP o mediante la aplicaci√≥n m√≥vil del banco BCP. Se recomienda esperar al menos 10 minutos despu√©s de generar la orden de pago antes de proceder con el pago. Si necesitas m√°s informaci√≥n, no dudes en preguntar.<|im_end|>\n",
      "<|im_start|>user\n",
      "¬øY qu√© pasa si no tengo cuenta en el BCP? ¬øHay otras opciones para pagar el autoseguro?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Lamentablemente, solo se menciona el pago del autoseguro a trav√©s de sucursales del BCP o mediante la aplicaci√≥n m√≥vil del mismo banco. No tengo acceso a informaci√≥n sobre otras opciones de pago disponibles. Te sugiero que contactes a la Oficina de Estad√≠stica de la Facultad de Ciencias para obtener m√°s detalles sobre posibles alternativas. Su correo es estadistica_fc@uni.edu.pe y su horario de atenci√≥n es de lunes a viernes de 8:00 a.m. a 4:00 p.m. Si necesitas m√°s ayuda, no dudes en preguntar.<|im_end|>\n",
      "<|im_start|>user\n",
      "Entendido. Voy a contactar a la Oficina de Estad√≠stica para ver si tienen m√°s opciones. ¬øSabes si hay alg√∫n plazo espec√≠fico para hacer el pago del autoseguro?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Los estudiantes deben realizar el pago del autoseguro estudiantil antes de las fechas de matr√≠cula, dentro del plazo m√°ximo establecido en el calendario de actividades acad√©micas correspondiente al periodo acad√©mico. Si no se cumple con este plazo, no se habilitar√° su matr√≠cula regular y deber√° gestionar la matr√≠cula como rezagada, lo que puede implicar el riesgo de no alcanzar vacantes para los cursos deseados. Te recomiendo revisar el calendario de actividades acad√©micas en la secci√≥n \"MATR√çCULA Y PROCEDIMIENTOS\" de la p√°gina web de la Facultad de Ciencias para obtener las fechas espec√≠ficas. Si necesitas m√°s ayuda, no dudes en preguntar.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb91b77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                         <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Si un alumno no cumpli√≥ con el pago del autoseguro, no podr√° realizar la matr√≠cula rezagada. Para gestionar su matr√≠cula rezagada, primero debe realizar el pago del autoseguro m√©dico estudiantil. Luego, deber√° seguir estos pasos:\n",
      "\n",
      "1. Generar la orden de pago por concepto de matr√≠cula rezagada a trav√©s del portal INTRALU y realizar el pago correspondiente de S/ 26 en una agencia del BCP o mediante su banca m√≥vil.\n",
      "2. Presentarse en la Oficina de Estad√≠stica (AERA) en las fechas establecidas en el calendario de actividades acad√©micas, llevando el comprobante de pago.\n",
      "\n",
      "Recuerde que la matr√≠cula rezagada se realiza solo si se cumplen todos los requisitos y en las fechas establecidas.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens= False).input_ids[0]\n",
    "print(tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf3a951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX 4500 Ada Generation. Max memory = 23.994 GB.\n",
      "0.807 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ed1bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,821 | Num Epochs = 5 | Total steps = 455\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 4 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 40,370,176/600,000,000 (6.73% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='455' max='455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [455/455 44:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>0.519353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>0.485724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.539941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.620500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>0.660862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b052bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Eres Aerito, un asistente de IA especializado en temas de matr√≠cula, procedimientos y tr√°mites acad√©micos de la Facultad de Ciencias de la Universidad Nacional de Ingenier√≠a del Per√∫.\n",
      "Deber√°s responder a los mensajes asegur√°ndote de cumplir con los siguientes criterios.\n",
      "    1. Proporcionar respuestas informativas, √∫tiles y concisas a las preguntas del usuario bas√°ndote exclusivamente en la informaci√≥n relacionada con la Facultad de Ciencias que conoces.\n",
      "    2. Mant√©n un tono cordial, emp√°tico y servicial en sus interacciones.\n",
      "    3. Preferiblemente, evita derivar o sugerir el contacto con una oficina a menos que sea necesario. Si no hay otra oficina m√°s id√≥nea, la derivaci√≥n se realizar√° hacia la Oficina de Estad√≠stica (AERA) de la Facultad de Ciencias.\n",
      "    4. En caso de no tener conocimiento sobre lo consultado, expresa con empat√≠a que no tienes acceso a dicha informaci√≥n.<|im_end|>\n",
      "<|im_start|>user\n",
      "como hago el retiro parcial<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Para solicitar el retiro parcial, debes seguir estos pasos a trav√©s de la plataforma INTRALU:\n",
      "\n",
      "1. Accede a INTRALU con tu c√≥digo UNI y clave.\n",
      "2. Selecciona \"Tr√°mites\" en el men√∫ de navegaci√≥n y elige \"Retiro Parcial\", hazendo clic en \"Nuevo\".\n",
      "3. Lee las instrucciones sobre el procedimiento y haz clic en \"Siguiente\".\n",
      "4. Consulta los requisitos: no mayor de seven (7) vecciones, y puedes retirar hasta tres (03) asignaturas.\n",
      "5. Elige las asignaturas para retirarte, asegur√°ndote de no tener cr√©ditos de otras cursos.\n",
      "\n",
      "Finalmente, confirma tu solicitud en \"Siguiente paso\" y espera el estado de tu tr√°mite. Recuerda que puedes retirar hasta tres asignaturas, pero no las que est√°n repetidas o desaprobadas dos o m√°s veces\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "#tokenizer = get_chat_template(\n",
    "#    tokenizer,\n",
    "#    chat_template = \"qwen-2.5\",\n",
    "#)\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompt_role_system},\n",
    "    {\"role\": \"user\", \"content\": \"como\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs , max_new_tokens = 200, use_cache = True, temperature = 1.5, min_p = 0.1)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
